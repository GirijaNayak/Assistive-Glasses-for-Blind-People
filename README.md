# ğŸ•¶ï¸ Assistive Glasses for Blind People

An AI-powered multimodal assistive system designed to help visually impaired individuals by combining:

- ğŸ¯ Object Detection (YOLOv8)
- âœ‹ Gesture Recognition (MobileNetV2)
- ğŸ”Š Environmental Sound Recognition (PANNs CNN14 + ESC-50)
- ğŸ§  Multimodal Fusion Layer

---

## ğŸš€ Project Overview

This project integrates computer vision and audio intelligence into a unified system that can:

- Detect real-world objects using a camera
- Recognize hand gestures for interaction
- Identify environmental sounds
- Fuse visual + audio information for contextual awareness

The goal is to simulate smart assistive glasses capable of real-time perception and feedback.

---

## ğŸ§  Models Used

### ğŸ¯ Object Detection
- YOLOv8 (Ultralytics)
- Custom training supported

### âœ‹ Gesture Recognition
- MobileNetV2 (Transfer Learning)
- HaGRID Classification Dataset

### ğŸ”Š Audio Recognition
- PANNs (CNN14)
- ESC-50 Environmental Sound Dataset

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ object_detection/
â”œâ”€â”€ gesture_module/
â”œâ”€â”€ audio_module/
â”œâ”€â”€ fusion_layer/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ Installation

```bash
git clone https://github.com/YOUR_USERNAME/Assistive-Glasses-for-Blind-People.git
cd Assistive-Glasses-for-Blind-People
pip install -r requirements.txt
